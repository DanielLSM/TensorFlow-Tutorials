{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tensorflow to the python shell!\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a node of a constant in tensorflow \n",
    "# tf.constant returns a tf.Tensor and defines a tf.Operation in the graph \n",
    "a = tf.constant(42,dtype=tf.int32,name='my_a',shape=(1,))\n",
    "b = tf.constant(42,dtype=tf.int32,name='my_b',shape=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a sum in tensorflow\n",
    "# will add a tf.Operation to the graph\n",
    "c = tf.add(a,b) \n",
    "# or c = a + b, tensorflow accepts a range of arithmetic and logical ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but what is c? c is still a tensor, tf.add() returns a tensor and\n",
    "# adds an operation to the graph\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to create a session in order to run a specific node in the\n",
    "# graph. Furthermore, we have been adding nodes to the \"default\" graph,\n",
    "# created when importing the tensorflow module, but we could've created\n",
    "# a different one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session object to run nodes in the graph\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we may run any node in the graph by using the tf.Session.run()\n",
    "# on any node we wish. \n",
    "# The nodes are the tensors and the operations the edges of the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('My value of a is {}, b is {} and the sum c is {}'.format(sess.run(a),sess.run(b),sess.run(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may close a session with sess.close() which will give us an error when trying to run a node\n",
    "sess.close()\n",
    "sess.run(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can open a different session with tf.Session() again.\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, we need to dive deeper in the tensorflow structure. Tensorflow was designed to have superior computational\n",
    "# efficienty at a cost of having to define a computational graph and associated variables properly\n",
    "# Let's see how many tensors and operations we have so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the tensors currently in our default graph\n",
    "tf.contrib.graph_editor.get_tensors(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And visualize each tf.Operation added to the graph\n",
    "sess.graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may notice that we have the same amount of tf.Tensors and tf.Operations.\n",
    "# The fact is, when creating anything in TensorFlow we always need a space in memory to physically have each tensor\n",
    "# And we also need a way to compute and feed each tensor onto other nodes, by defining the operations (edges)\n",
    "# Let's save our graph and visualize it on tensorboard!\n",
    "tf.train.write_graph(tf.get_default_graph(), 'folder', 'our_graph.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, we may reset the graph and all the nodes we created, since jupyter notebook is an interactive\n",
    "# python shell, remenber to reset the graph from time to time :>\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And visualize there are no tensors or operations in our graph\n",
    "tf.contrib.graph_editor.get_tensors(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the important things to be adressed. Everytime we call a tf function\n",
    "# we create a tensor (and consequently an operation), this leads to subtle memory leaks.\n",
    "for i in range(5):\n",
    "    a = tf.constant(42,dtype=tf.int32,name='my_a',shape=(1,))\n",
    "tf.contrib.graph_editor.get_tensors(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# When desiging graphs (such as neural networks) it is a good practice to define all the nodes at start\n",
    "# and running only the nodes in our code\n",
    "a = tf.constant(42,dtype=tf.int32,name='my_a',shape=(1,))\n",
    "for i in range(5):\n",
    "    print(sess.run(a)) \n",
    "tf.contrib.graph_editor.get_tensors(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy Reset\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may create tf.Variables, which will hold a variable Tensor during computations\n",
    "# Most nodes will be composed of tf.Variables.\n",
    "# There are two ways to define new variables in TensorFlow, by creating a \n",
    "# tf.Variable object or by calling tf.get_variable. \n",
    "# Calling tf.get_variable with a new name results in creating a new variable, \n",
    "# but if a variable with the same name exists it \n",
    "# will raise a ValueError exception, telling us that re-declaring \n",
    "# a variable is not allowed.\n",
    "d = tf.get_variable('my_d',[1,2,3]) #Here we already defined the shape 1-D array\n",
    "e = tf.get_variable('my_e', shape=(18,)) #Here the shape an 1-D array\n",
    "f = tf.get_variable('my_f', dtype=tf.int32, initializer=tf.constant([23, 42])) \n",
    "#Here we defined the type and the initializer which happens to be a constant tf.constant, the shape is defined\n",
    "tf.contrib.graph_editor.get_tensors(tf.get_default_graph())\n",
    "# Why are there so many tensors? the second argument is the shape. The default initializer of variables\n",
    "# is a random uniform distribution. Each Tensor of this Initializer is also created under the name of each variable\n",
    "# Notice the Tensors Assign and read, serve to feed and fetch values from the Tensor 'my_variable0':0\n",
    "# In the 'my_f' variable we only have 4 tensors, the initializer, which is a tf.constant, the \"de facto\" variable\n",
    "# 'my_f:0' and 2 other tensors to read and assing values to 'my_f:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the session again, instead of creating a new tf.Session, this is handy on long, complex programs.\n",
    "# If there isn't any tf.Session available, it will return 'None', so we need to create another session.\n",
    "# sess = tf.Session()\n",
    "sess = tf.get_default_session()\n",
    "# Run the node f and print it\n",
    "sess.run(f)\n",
    "# Why did we got an error? the tf.initalizer is a method, and has not been run, therefore\n",
    "# there are no values to be retrieved when we run the node. We have two options: \n",
    "# 1) Either run a particular variable initializer with sess.run(f.initializer)\n",
    "# 2) Or run the tf.global_variables_initializer() which will initialize all the nodes created in the graph thus far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "# Every node (tensor) when runned, return an numpy array\n",
    "sess.run(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# So far we have seen tf.constant and tf.get_variable. However there is also a data structure appropriate to \n",
    "# feed data into our models. We could feed them through variables, however variables are more heavy and have\n",
    "# initializers, read, write tensors... it is a good practice to use tf.placeholders on entry nodes of our model\n",
    "# since they are lightweight and specifically suited for this task\n",
    "g = tf.placeholder(tf.float32, shape=(1,),name='entrada')\n",
    "tf.contrib.graph_editor.get_tensors(tf.get_default_graph())\n",
    "# As we can see, placeholders are very light weight, holding only 1 tensor\n",
    "# using them to feed data into our model is extremely clean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'x_1:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'Const:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'x_2:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'x_2/Assign:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'x_2/read:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'add:0' shape=() dtype=int32>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# How do we run a specific node while feeding data at the same time?\n",
    "# On of the arguments of tf.Session.run(node,feed_dict) is feed_dict\n",
    "# which is a python dictionary (also obeys the same sintax of json).\n",
    "# Thus, we run a node, while feeding certain dependant nodes.\n",
    "# For example, let us run a sum with a variable and a placeholder\n",
    "x_1 = tf.placeholder(tf.int32, (),'x_1')\n",
    "x_2 = tf.get_variable('x_2', dtype=tf.int32 ,initializer=tf.constant(21))\n",
    "x_result = x_1 + x_2 #this adition will make one tensor and one operation\n",
    "tf.contrib.graph_editor.get_tensors(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets run the node x_result with a custom x_1, after initializing\n",
    "# x_2!\n",
    "sess = tf.Session()\n",
    "# sess = tf.get_default_session()\n",
    "sess.run(x_2.initializer)\n",
    "# Create a dictionary to feed x_1\n",
    "dictionary = {x_1 : 21}\n",
    "# Run x_result node, while \n",
    "sess.run(x_result,feed_dict=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can, however, feed variables aswell (placeholder always need to be fed)\n",
    "sess.run(x_result,feed_dict={x_1:21,x_2:21})\n",
    "# Another important note: \n",
    "# shape=() is a single number 0-D\n",
    "# shape = (1,) is a 1-D, usual python list\n",
    "# shape = (1,1) is a 2-D, a python list of a list with a single point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'sum/Const:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'sum/x_1:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'sum/x_1/Assign:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'sum/x_1/read:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'sum/Const_1:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'sum/x_2:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'sum/x_2/Assign:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'sum/x_2/read:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'sum/Add:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'subtract/Const:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'subtract/x_1:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'subtract/x_1/Assign:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'subtract/x_1/read:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'subtract/Const_1:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'subtract/x_2:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'subtract/x_2/Assign:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'subtract/x_2/read:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'subtract/Sub:0' shape=() dtype=int32>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We may group variables, placeholders and constants into a \n",
    "# name scope. This is useful when we want to handle a group of \n",
    "# variables, weights at the same time, such as initializing them\n",
    "# with the same distribution, or saving them into a particular place.\n",
    "# It also makes the graph well organized and easier to observe\n",
    "# and debug in tensorboard :>\n",
    "# Lets create 2 scopes, one for sum and one for subtraction\n",
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('sum') as scope:\n",
    "    x_1 = tf.get_variable('x_1',dtype=tf.int32,initializer=tf.constant(21))\n",
    "    x_2 = tf.get_variable('x_2',dtype=tf.int32,initializer=tf.constant(21))\n",
    "    x_sum = tf.add(x_1,x_2) #or x_1 + x_2\n",
    "\n",
    "with tf.variable_scope('subtract'):\n",
    "    x_1 = tf.get_variable('x_1',dtype=tf.int32,initializer=tf.constant(84))\n",
    "    x_2 = tf.get_variable('x_2',dtype=tf.int32,initializer=tf.constant(42))\n",
    "    x_subtract = tf.subtract(x_1,x_2) #or x_1 - x_2\n",
    "tf.contrib.graph_editor.get_tensors(tf.get_default_graph())\n",
    "# As we can see we have 2 sets of tensors, one grouped under the name 'sum' \n",
    "# and another one grouped at 'subtract'. Furthermore, one might say we have\n",
    "# in TensorFlow language, 2 name_scopes each one with a collection of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum is 42 and the subtraction is 42\n"
     ]
    }
   ],
   "source": [
    "# Running the 2 operations is easy. We initialize variables and run the result nodes\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('The sum is {} and the subtraction is {}'.format(sess.run(x_sum),sess.run(x_subtract)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'sum/x_1:0' shape=() dtype=int32_ref>,\n",
       " <tf.Variable 'sum/x_2:0' shape=() dtype=int32_ref>,\n",
       " <tf.Variable 'subtract/x_1:0' shape=() dtype=int32_ref>,\n",
       " <tf.Variable 'subtract/x_2:0' shape=() dtype=int32_ref>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We may only look to one set of variables from one specific selection,\n",
    "# tf.Variables hold tensors+operations, we may select a set to reuse,\n",
    "# or to save. \n",
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'sum/x_1:0' shape=() dtype=int32_ref>,\n",
       " <tf.Variable 'sum/x_2:0' shape=() dtype=int32_ref>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or under a specific scope\n",
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/daniel/TensorFlow-Tutorials']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=!pwd\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/daniel/TensorFlow-Tutorials/model/model'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We may save our session, to continue it later. This will freeze every tensor\n",
    "# assigned inner value and save in an appropriate file to be restored.\n",
    "saver = tf.train.Saver()\n",
    "path = !pwd # path to this folder on your computer, ! computes in bash\n",
    "saver.save(sess,path[0]+\"/model/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sums has 2 objects and x_sum is 42 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'sum/Const:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'sum/x_1:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'sum/x_1/Assign:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'sum/x_1/read:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'sum/Const_1:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'sum/x_2:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'sum/x_2/Assign:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'sum/x_2/read:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'sum/Add:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'subtract/Const:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'subtract/x_1:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'subtract/x_1/Assign:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'subtract/x_1/read:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'subtract/Const_1:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'subtract/x_2:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'subtract/x_2/Assign:0' shape=() dtype=int32_ref>,\n",
       " <tf.Tensor 'subtract/x_2/read:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'subtract/Sub:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'add/y:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'add:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'add_1/y:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'add_1:0' shape=() dtype=int32>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For instance, we may use all variables from a scope\n",
    "sums = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope='sum') #List of Variables\n",
    "# For every tf.Variable, we will add 1\n",
    "sumcity = []\n",
    "for var in sums:\n",
    "    sumcity.append(var + 1)\n",
    "\n",
    "sumscity = tf.group(*sumcity)    \n",
    "#print(sess.run(var))\n",
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope='sum')\n",
    "print('sums has {} objects and x_sum is {} '.format(len(sums),sess.run(x_sum)))\n",
    "tf.contrib.graph_editor.get_tensors(tf.get_default_graph())\n",
    "# If the graph is leaking because you are running this several times, reset it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22, 22]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(sumcity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.graph.get_tensor_by_name('my_a:0')\n",
    "\n",
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "\n",
    "g = tf.get_variable('my_d') #Here we use the same tensor name 'my_d' created in d to perform\n",
    "# a different computation with the same weights on another part of the program\n",
    "\n",
    "When we run a particular node,  \n",
    "\n",
    "sess.run(a)\n",
    "\n",
    "#feed and run, save and restore models\n",
    "\n",
    "session.run(b)\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RG]",
   "language": "python",
   "name": "conda-env-RG-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
